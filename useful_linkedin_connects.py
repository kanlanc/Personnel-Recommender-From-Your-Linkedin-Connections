# -*- coding: utf-8 -*-
"""Useful Linkedin Connects.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1onyGXzQzMW8exK4rHVQ9WezidUxEAqLI
"""

!apt install -y chromium-chromedriver
!pip install selenium

!pip install modal
!pip install -U langchain
!pip install anthropic
!pip install cohere

import os
os.environ['username'] = 'itsmethefounder@outlook.com'
os.environ['password'] = "Tech4Life!"
os.environ['ANTHROPIC_API_KEY'] = "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"
os.environ['LANGCHAIN_API_KEY'] = "ls__0aa97ffdedf342068430ab83273564fd"


openai_api_key = ""
ANTHROPIC_API_KEY=""
SERPAPI_API_KEY=""


LANGCHAIN_API_KEY="ls__0aa97ffdedf342068430ab83273564fd"
LANGCHAIN_TRACING_V2=True
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_PROJECT="Foodsmith"


cohere_api_key = "5GIQYhLSWrnXOprlPqJSwKu6l7awxtBfi26R9c7c"

import zipfile
import pandas as pd

# # Extract the ZIP file
# with zipfile.ZipFile('data.zip', 'r') as zip_ref:
#     zip_ref.extractall()

# Read the CSV file from the extracted contents
df = pd.read_csv('Linkedin Connections Data.csv')

# Display the first few rows of the DataFrame
df.head(10)





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import time

# chrome_options = Options()
# chrome_options.add_argument("--headless")
# chrome_options.add_argument("--no-sandbox")
# chrome_options.add_argument("--disable-dev-shm-usage")



# driver = webdriver.Chrome(options=chrome_options)

# # URL of the profile you want to scrape
# url = 'https://www.linkedin.com/in/alex-poe-79a049a1'

# # Wait for the page to load
# try:
#     element = WebDriverWait(driver, 15).until(
#         EC.presence_of_element_located((By.ID, "experience"))
#     )
# except TimeoutException:
#     print("Timed out waiting for page to load")
#     driver.quit()

# # Get the page source
# page_source = driver.page_source

# # Parse the page source with BeautifulSoup
# soup = BeautifulSoup(page_source, 'html.parser')

# # Find the "div" with id "experience"
# experience_div = soup.find('div', {'id': 'experience'})

# # Find the closest parent "section" to this "div"
# if experience_div:
#     experience_section = experience_div.find_parent('section')
#     # Continue with the extraction as before
#     if experience_section:
#         for item in experience_section.find_all('li', {'class': 'pv-entity'}):  # Update class based on actual HTML structure
#             job_title = item.find('h3')  # Update based on actual HTML structure
#             company = item.find('p', {'class': 'pv-entity__secondary-title'})  # Update based on actual HTML structure
#             duration = item.find('h4', {'class': 'pv-entity__date-range'})  # Update based on actual HTML structure

#             if job_title:
#                 job_title = job_title.text.strip()
#             if company:
#                 company = company.text.strip()
#             if duration:
#                 duration = duration.text.strip()

#             print(f"Job Title: {job_title}")
#             print(f"Company: {company}")
#             print(f"Duration: {duration}")
#             print('-' * 40)
#     else:
#         print("Experience section not found.")
# else:
#     print("Div with id 'experience' not found.")

# # Close the driver
# driver.quit()

# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from bs4 import BeautifulSoup
# import time

# # Creating a webdriver instance

# chrome_options = Options()
# chrome_options.add_argument("--headless")
# chrome_options.add_argument("--no-sandbox")
# chrome_options.add_argument("--disable-dev-shm-usage")

# # Mimic a real browser
# chrome_options.add_argument("start-maximized")
# chrome_options.add_argument("disable-infobars")
# chrome_options.add_argument("--disable-extensions")
# chrome_options.add_argument("--disable-blink-features=AutomationControlled")
# chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537")




# driver = webdriver.Chrome(options=chrome_options)

# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from bs4 import BeautifulSoup
# import time

# # Creating an instance

# # Logging into LinkedIn
# driver.get("https://linkedin.com/uas/login")
# time.sleep(5)

# username = driver.find_element(By.ID, "username")
# username.send_keys(os.environ['username'])  # Enter Your Email Address

# pword = driver.find_element(By.ID, "password")
# pword.send_keys(os.environ['password'])        # Enter Your Password

# driver.find_element(By.XPATH, "//button[@type='submit']").click()

# # Opening Kunal's Profile
# # paste the URL of Kunal's profile here
# profile_url = "https://www.linkedin.com/in/rondovfisher"

# driver.get(profile_url)        # this will open the link

# start = time.time()

# # will be used in the while loop
# initialScroll = 0
# finalScroll = 1000

# while True:
# 	driver.execute_script(f"window.scrollTo({initialScroll},{finalScroll})")
# 	# this command scrolls the window starting from
# 	# the pixel value stored in the initialScroll
# 	# variable to the pixel value stored at the
# 	# finalScroll variable
# 	initialScroll = finalScroll
# 	finalScroll += 1000

# 	# we will stop the script for 3 seconds so that
# 	# the data can load
# 	time.sleep(3)
# 	# You can change it as per your needs and internet speed

# 	end = time.time()

# 	# We will scroll for 20 seconds.
# 	# You can change it as per your needs and internet speed
# 	if round(end - start) > 20:
# 		break


# src = driver.page_source
# # Now using beautiful soup
# soup = BeautifulSoup(src, 'lxml')

# # Getting the HTML of the Experience section in the profile
# experience = soup.find("section", {"id": "experience-section"}).find('ul')

# print(experience)

from langchain.chat_models import ChatAnthropic
# from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.agents import Tool
from langchain.agents import AgentType
from langchain.utilities import SerpAPIWrapper
from langchain.agents import initialize_agent
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.docstore.document import Document
import requests
from langchain.prompts.chat import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain import PromptTemplate
from langchain import LLMChain
import cohere

co = cohere.Client(cohere_api_key)
claude = ChatAnthropic(temperature=0)
cohere = ChatAnthropic(temperature=0)
gpt = ChatAnthropic(temperature=0)

user_connections_list = pd.read_csv("enriched_data_linkedin_data_connections.csv")
user_query = "Give me people who worked in the XR industry"

template = """Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".

Context: You are a personnel recommending agent. Given the user required personnel, find the top 4 people with relevant experience
and background that fit the user's needs in a table format with also a score on the right on how much they match.

You will do this analysis from the user's list available below

User List: {user_connections_list}

User Query : {user_query}

Answer: """

prompt_template = PromptTemplate(
    input_variables=["user_connections_list","user_query"],
    template=template
)



claude_chain = LLMChain(prompt=prompt_template, llm=claude)

claude_output = claude_chain.run(user_connections_list=user_connections_list,user_query=user_query)

print(claude_output)